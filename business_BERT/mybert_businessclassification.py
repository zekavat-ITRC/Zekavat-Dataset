# -*- coding: utf-8 -*-
"""MyBERT_businessClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wDq83IcvMsOwF7BkIFQApJSUPVFoSN2i

## PyTorch
"""

# Install required packages

# !pip install -q transformers
# !pip install -q hazm
# !pip install -q clean-text[gpl]

# Import required packages
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
# from sklearn.utils import shuffle

import hazm
from cleantext import clean

# import plotly.express as px
# import plotly.graph_objects as go

# from tqdm.notebook import tqdm
from tqdm import tqdm

# import os
import re
import json
# import copy
import collections

# import pandas as pd


from transformers import BertConfig, BertTokenizer
from transformers import BertModel

# from torch.optim import AdamW  # from transformers import AdamW
from transformers import AdamW
from transformers import get_linear_schedule_with_warmup

import torch
import torch.nn as nn
import torch.nn.functional as F


def cleanhtml(raw_html):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext


def cleaning(text):
    text = text.strip()

    # regular cleaning
    text = clean(text,
                 fix_unicode=True,
                 to_ascii=False,
                 lower=True,
                 no_line_breaks=True,
                 no_urls=True,
                 no_emails=True,
                 no_phone_numbers=True,
                 no_numbers=False,
                 no_digits=False,
                 no_currency_symbols=True,
                 no_punct=False,
                 replace_with_url="",
                 replace_with_email="",
                 replace_with_phone_number="",
                 replace_with_number="",
                 replace_with_digit="0",
                 replace_with_currency_symbol="",
                 )

    # cleaning htmls
    text = cleanhtml(text)

    # normalizing
    normalizer = hazm.Normalizer()
    text = normalizer.normalize(text)

    # removing wierd patterns
    wierd_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u'\U00010000-\U0010ffff'
                               u"\u200d"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\u3030"
                               u"\ufe0f"
                               u"\u2069"
                               u"\u2066"
                               # u"\u200c"
                               u"\u2068"
                               u"\u2067"
                               "]+", flags=re.UNICODE)

    text = wierd_pattern.sub(r' ', text)

    # removing extra spaces, hashtags
    text = re.sub("#", " ", text)
    text = re.sub("_", " ", text)
    text = re.sub("\s+", " ", text)

    return text



# if os.path.exists('df_all_cleaned.xlsx'):
#     data_test = pd.read_excel('df_all_cleaned.xlsx')
# else:
#     data_test = pd.read_csv('df_dataset_dakheli_23_class.csv')
#     data_test['newLabel'] = data_test['label']  # .astype(str)
#     data_test['caption'] = data_test['txtContent'].astype(str)
#     data_test['cleaned_text'] = data_test['caption'].apply(cleaning)
#     data_test.to_excel('df_all_cleaned.xlsx', index=False)

id2label = {
    0: 'تشریفات و نظافت',
    1: 'رستوران، آشپزی و قنادی',  ################################## az baghiye lbls estefade kon
    2: 'خودرو و موتور',
    3: 'تعمیرات،ابزارآلات و مصالح ساختمانی',
    4: 'گردشگری و مهاجرت',
    5: 'کالا و خدمات ورزشی',
    6: 'کتاب و لوازم تحریر',
    7: 'طالع بینی',
    8: 'غیر اخلاقی وغیر قانونی',
    9: 'بورس و ارز دیجیتال',
    10: 'مشاوره - آموزشی',
    11: 'آرایشی و بهداشتی',
    12: 'پزشکی-درمانی',
    13: 'اکسسوری و زیورآلات',
    14: 'پوشاک',
    15: 'کیف و کفش',
    16: 'عروسک و اسباب بازی',
    17: 'گل و گیاه',
    18: 'شرط بندی',
    19: 'پت شاپ',
    20: 'کالا و خدمات دیجیتال',
    21: 'لوازم خانه و آشپزخانه',
    22: 'هنر',
    23: 'متفرقه',
}

label2id = {v: k for k, v in id2label.items()}

print(f'label2id: {label2id}')
print(f'id2label: {id2label}')



if os.path.exists('df_all_cleaned.xlsx'):
    data = pd.read_excel('df_all_cleaned.xlsx')
else:
    data = pd.read_csv('df_dataset_dakheli_24_class.csv')

    data['newLabel'] = data['label']  # .astype(str)
    data['caption'] = data['txtContent'].astype(str)

    # cleaning comments
    data['cleaned_text'] = data['caption'].apply(cleaning)

    # # calculate the length of comments based on their words
    # data['cleaned_text_len_by_words'] = data['cleaned_text'].apply(lambda t: len(hazm.word_tokenize(t)))

    # data = data.reset_index(drop=True)

    print(data.head())



    # data['newLabel'] = data['newLabel'].apply(lambda t: label2id[t])

    print(data.newLabel.value_counts())
    data = data[data.groupby('newLabel').newLabel.transform('count') > 7]  # .copy()
    print(data.newLabel.value_counts())

    print(data.iloc[0]['newLabel'].dtype)

    data.to_excel('df_all_cleaned.xlsx', index=False)

# new_data['label_id'] = new_data['label'].apply(lambda t: labels.index(t))

train, test = train_test_split(data, test_size=0.1, random_state=42, stratify=data['newLabel'])
valid = test.copy()


# train = data.copy()
# test = data_test.copy()
# valid = test.copy()

print(train.newLabel.value_counts())
print(test.newLabel.value_counts())

# train = train.reset_index(drop=True)
# valid = valid.reset_index(drop=True)
# test = test.reset_index(drop=True)

x_train, y_train = train['cleaned_text'].values.tolist(), train['newLabel'].values.tolist()
x_valid, y_valid = valid['cleaned_text'].values.tolist(), valid['newLabel'].values.tolist()
x_test, y_test = test['cleaned_text'].values.tolist(), test['newLabel'].values.tolist()

print(train.shape)
print(valid.shape)
print(test.shape)
# y_train[0].dtype
# y_valid[0].dtype
# y_test[0].dtype


"""### Configuration"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f'device: {device}')

train_on_gpu = torch.cuda.is_available()

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')

# general config
MAX_LEN = 128
TRAIN_BATCH_SIZE = 16
VALID_BATCH_SIZE = 16
TEST_BATCH_SIZE = 16

EPOCHS = 4  # 3
EEVERY_EPOCH = len(x_train) // TRAIN_BATCH_SIZE  # 1000
if len(x_train) % TRAIN_BATCH_SIZE != 0:
    EEVERY_EPOCH += 1

LEARNING_RATE = 2e-5
CLIP = 0.0

MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'
OUTPUT_PATH = './bert-fa-base-uncased-sentiment-taaghceh/pytorch_model.bin'

os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)

# setup the tokenizer and configuration

tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)
config = BertConfig.from_pretrained(
    MODEL_NAME_OR_PATH, **{
        'label2id': label2id,
        'id2label': id2label,
    })

# print(config.to_json_string())

"""### Input Embeddings"""

idx = np.random.randint(0, len(train))
sample_comment = train.iloc[idx]['cleaned_text']
sample_label = train.iloc[idx]['newLabel']

print(f'Samp{sample_label}\n{id2label[sample_label]}')

tokens = tokenizer.tokenize(sample_comment)
token_ids = tokenizer.convert_tokens_to_ids(tokens)

print(f'  Comment: {sample_comment}')
print(f'   Tokens: {tokenizer.convert_tokens_to_string(tokens)}')
print(f'Token IDs: {token_ids}')

encoding = tokenizer.encode_plus(
    sample_comment,
    max_length=32,
    truncation=True,
    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
    return_token_type_ids=True,
    return_attention_mask=True,
    padding='max_length',
    return_tensors='pt',  # Return PyTorch tensors
)

print(f'Keys: {encoding.keys()}\n')
for k in encoding.keys():
    print(f'{k}:\n{encoding[k]}')

"""### Dataset"""


class BusinessDataset(torch.utils.data.Dataset):
    """ Create a PyTorch dataset for BusinessDataset. """

    def __init__(self, tokenizer, comments, targets=None, label_list=None, max_len=128):
        self.comments = comments
        self.targets = targets
        self.has_target = isinstance(targets, list) or isinstance(targets, np.ndarray)

        self.tokenizer = tokenizer
        self.max_len = max_len

        self.label_map = label2id  # {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}

    def __len__(self):
        return len(self.comments)

    def __getitem__(self, item):
        comment = str(self.comments[item])

        if self.has_target:
            target = self.targets[item]

        encoding = self.tokenizer.encode_plus(
            comment,
            add_special_tokens=True,
            truncation=True,
            max_length=self.max_len,
            return_token_type_ids=True,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt')

        inputs = {
            'comment': comment,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'token_type_ids': encoding['token_type_ids'].flatten(),
        }

        if self.has_target:
            inputs['targets'] = torch.tensor(target, dtype=torch.long)

        return inputs


def create_data_loader(x, y, tokenizer, max_len, batch_size, label_list, shuffle):
    dataset = BusinessDataset(
        comments=x,
        targets=y,
        tokenizer=tokenizer,
        max_len=max_len,
        label_list=label_list)

    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


print(MAX_LEN)

label_list = list(label2id.keys())  # ['negative', 'positive']
train_data_loader = create_data_loader(train['cleaned_text'].to_numpy(), train['newLabel'].to_numpy(), tokenizer,
                                       MAX_LEN, TRAIN_BATCH_SIZE, label_list, shuffle=True)
valid_data_loader = create_data_loader(valid['cleaned_text'].to_numpy(), valid['newLabel'].to_numpy(), tokenizer,
                                       MAX_LEN, VALID_BATCH_SIZE, label_list, shuffle=False)
test_data_loader = create_data_loader(test['cleaned_text'].to_numpy(), None, tokenizer, MAX_LEN, TEST_BATCH_SIZE,
                                      label_list, shuffle=False)

sample_data = next(iter(train_data_loader))

print(sample_data.keys())

print(sample_data['comment'])
print(sample_data['input_ids'].shape)
print(sample_data['input_ids'][0, :])
print(sample_data['attention_mask'].shape)
print(sample_data['attention_mask'][0, :])
print(sample_data['token_type_ids'].shape)
print(sample_data['token_type_ids'][0, :])
print(sample_data['targets'].shape)
print(sample_data['targets'][0])

sample_test = next(iter(test_data_loader))
print(sample_test.keys())

"""### Model

During the implementation of the model, sometime, you may be faced with this kind of error. It said you used all the Cuda-Memory for solving. There are many ways for the simple one is to clear the Cuda cache memory!

![Cuda-Error](https://res.cloudinary.com/m3hrdadfi/image/upload/v1599979552/kaggle/cuda-error_iyqh4o.png)


**Simple Solution**
```python
import torch, gc

gc.collect()
torch.cuda.empty_cache()

!nvidia-smi
```
"""


class SentimentModel(nn.Module):

    def __init__(self, config):
        super(SentimentModel, self).__init__()

        self.bert = BertModel.from_pretrained(MODEL_NAME_OR_PATH, return_dict=False)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, input_ids, attention_mask, token_type_ids):
        _, pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids)

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits


import torch, gc

gc.collect()
torch.cuda.empty_cache()
pt_model = None

# !nvidia-smi

pt_model = SentimentModel(config=config)
# pt_model.load_state_dict(torch.load('bert_business0.pt'))
pt_model = pt_model.to(device)

print('pt_model', type(pt_model))

# sample data output

sample_data_comment = sample_data['comment']
sample_data_input_ids = sample_data['input_ids']
sample_data_attention_mask = sample_data['attention_mask']
sample_data_token_type_ids = sample_data['token_type_ids']
sample_data_targets = sample_data['targets']

# available for using in GPU
sample_data_input_ids = sample_data_input_ids.to(device)
sample_data_attention_mask = sample_data_attention_mask.to(device)
sample_data_token_type_ids = sample_data_token_type_ids.to(device)
sample_data_targets = sample_data_targets.to(device)

# outputs = F.softmax(
#     pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids), 
#     dim=1)

outputs = pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids)
_, preds = torch.max(outputs, dim=1)

print(outputs[:5, :])
print(preds[:5])

"""### Training"""


def simple_accuracy(y_true, y_pred):
    return (y_true == y_pred).mean()


def acc_and_f1(y_true, y_pred, average='weighted'):
    acc = simple_accuracy(y_true, y_pred)
    f1 = f1_score(y_true=y_true, y_pred=y_pred, average=average)
    return {
        "acc": acc,
        "f1": f1,
    }


def y_loss(y_true, y_pred, losses):
    y_true = torch.stack(y_true).cpu().detach().numpy()
    y_pred = torch.stack(y_pred).cpu().detach().numpy()
    y = [y_true, y_pred]
    loss = np.mean(losses)

    return y, loss


def eval_op(model, data_loader, loss_fn):
    model.eval()

    losses = []
    y_pred = []
    y_true = []

    with torch.no_grad():
        for dl in tqdm(data_loader, total=len(data_loader), desc="Evaluation... "):
            input_ids = dl['input_ids']
            attention_mask = dl['attention_mask']
            token_type_ids = dl['token_type_ids']
            targets = dl['targets']

            # move tensors to GPU if CUDA is available
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            token_type_ids = token_type_ids.to(device)
            targets = targets.to(device)

            # compute predicted outputs by passing inputs to the model
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids)

            # convert output probabilities to predicted class
            _, preds = torch.max(outputs, dim=1)

            # calculate the batch loss
            loss = loss_fn(outputs, targets)

            # accumulate all the losses
            losses.append(loss.item())

            y_pred.extend(preds)
            y_true.extend(targets)

    eval_y, eval_loss = y_loss(y_true, y_pred, losses)
    return eval_y, eval_loss


def train_op(model,
             data_loader,
             loss_fn,
             optimizer,
             scheduler,
             step=0,
             print_every_step=100,
             eval=False,
             eval_cb=None,
             eval_loss_min=np.Inf,
             eval_data_loader=None,
             clip=0.0):
    model.train()

    losses = []
    y_pred = []
    y_true = []

    for dl in tqdm(data_loader, total=len(data_loader), desc="Training... "):
        step += 1

        input_ids = dl['input_ids']
        attention_mask = dl['attention_mask']
        token_type_ids = dl['token_type_ids']
        targets = dl['targets']

        # move tensors to GPU if CUDA is available
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        token_type_ids = token_type_ids.to(device)
        targets = targets.to(device)

        # clear the gradients of all optimized variables
        optimizer.zero_grad()

        # compute predicted outputs by passing inputs to the model
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids)

        # convert output probabilities to predicted class
        _, preds = torch.max(outputs, dim=1)

        # calculate the batch loss
        loss = loss_fn(outputs, targets)

        # accumulate all the losses
        losses.append(loss.item())

        # compute gradient of the loss with respect to model parameters
        loss.backward()

        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
        if clip > 0.0:
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)

        # perform optimization step
        optimizer.step()

        # perform scheduler step
        scheduler.step()

        y_pred.extend(preds)
        y_true.extend(targets)

        if eval:
            train_y, train_loss = y_loss(y_true, y_pred, losses)
            train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')

            if step % print_every_step == 0:
                eval_y, eval_loss = eval_op(model, eval_data_loader, loss_fn)
                eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')

                if hasattr(eval_cb, '__call__'):
                    eval_loss_min = eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min)

    train_y, train_loss = y_loss(y_true, y_pred, losses)

    return train_y, train_loss, step, eval_loss_min


import torch, gc

# del outputs, preds
# del sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids, sample_data_targets
# del pt_model

gc.collect()
# pt_model = None

# torch.cuda.empty_cache()

# !nvidia-smi

optimizer = AdamW(pt_model.parameters(), lr=LEARNING_RATE, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

loss_fn = nn.CrossEntropyLoss()

step = 0
eval_loss_min = np.Inf
history = collections.defaultdict(list)


def eval_callback(epoch, epochs, output_path):
    def eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min):
        statement = ''
        statement += 'Epoch: {}/{}...'.format(epoch, epochs)
        statement += 'Step: {}...'.format(step)

        statement += 'Train Loss: {:.6f}...'.format(train_loss)
        statement += 'Train Acc: {:.3f}...'.format(train_score['acc'])

        statement += 'Valid Loss: {:.6f}...'.format(eval_loss)
        statement += 'Valid Acc: {:.3f}...'.format(eval_score['acc'])

        print(statement)

        if eval_loss <= eval_loss_min:
            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(
                eval_loss_min,
                eval_loss))

            torch.save(model.state_dict(), output_path)
            torch.save(model.state_dict(), 'bert_business_24class_epoch.pt')
            eval_loss_min = eval_loss

        return eval_loss_min

    return eval_cb


for epoch in tqdm(range(1, EPOCHS + 1), desc="Epochs... "):
    train_y, train_loss, step, eval_loss_min = train_op(
        model=pt_model,
        data_loader=train_data_loader,
        loss_fn=loss_fn,
        optimizer=optimizer,
        scheduler=scheduler,
        step=step,
        print_every_step=EEVERY_EPOCH,
        eval=True,
        eval_cb=eval_callback(epoch, EPOCHS, OUTPUT_PATH),
        eval_loss_min=eval_loss_min,
        eval_data_loader=valid_data_loader,
        clip=CLIP)

    train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')

    eval_y, eval_loss = eval_op(
        model=pt_model,
        data_loader=valid_data_loader,
        loss_fn=loss_fn)

    eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')

    history['train_acc'].append(train_score['acc'])
    history['train_loss'].append(train_loss)
    history['val_acc'].append(eval_score['acc'])
    history['val_loss'].append(eval_loss)

"""### Prediction"""


def predict(model, comments, tokenizer, max_len=128, batch_size=32):
    data_loader = create_data_loader(comments, None, tokenizer, max_len, batch_size, None, shuffle=False)

    predictions = []
    prediction_probs = []

    model.eval()
    with torch.no_grad():
        for dl in tqdm(data_loader, position=0):
            input_ids = dl['input_ids']
            attention_mask = dl['attention_mask']
            token_type_ids = dl['token_type_ids']

            # move tensors to GPU if CUDA is available
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            token_type_ids = token_type_ids.to(device)

            # compute predicted outputs by passing inputs to the model
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids)

            # convert output probabilities to predicted class
            _, preds = torch.max(outputs, dim=1)

            predictions.extend(preds)
            prediction_probs.extend(F.softmax(outputs, dim=1))

    predictions = torch.stack(predictions).cpu().detach().numpy()
    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()

    return predictions, prediction_probs


test_comments = test['cleaned_text'].to_numpy()
preds, probs = predict(pt_model, test_comments, tokenizer, max_len=128)

print(preds.shape, probs.shape)

print(label_list)

# y_test, y_pred = [label_list.index(label) for label in test['newLabel'].values], preds
y_test, y_pred = [label for label in test['newLabel'].values], preds

print(f'F1: {f1_score(y_test, y_pred, average="weighted")}')
print()
print(classification_report(y_test, y_pred))  # , target_names=label_list))

# """## TensorFlow"""
#
# from transformers import BertConfig, BertTokenizer
# from transformers import TFBertModel, TFBertForSequenceClassification
# from transformers import glue_convert_examples_to_features
#
# import tensorflow as tf
#
# """### Configuration"""
#
# # general config
# MAX_LEN = 128
# TRAIN_BATCH_SIZE = 16
# VALID_BATCH_SIZE = 16
# TEST_BATCH_SIZE = 16
#
# EPOCHS = 3
# EEVERY_EPOCH = 1000
# LEARNING_RATE = 2e-5
# CLIP = 0.0
#
# MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'
# OUTPUT_PATH = '/content/bert-fa-base-uncased-sentiment-taaghceh/pytorch_model.bin'
#
# os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
#
# label2id = {label: i for i, label in enumerate(labels)}
# id2label = {v: k for k, v in label2id.items()}
#
# print(f'label2id: {label2id}')
# print(f'id2label: {id2label}')
#
# tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)
# config = BertConfig.from_pretrained(
#     MODEL_NAME_OR_PATH, **{
#         'label2id': label2id,
#         'id2label': id2label,
#     })
#
# print(config.to_json_string())
#
# """### Input Embeddings / Dataset"""
#
# class InputExample:
#     """ A single example for simple sequence classification. """
#
#     def __init__(self, guid, text_a, text_b=None, label=None):
#         """ Constructs a InputExample. """
#         self.guid = guid
#         self.text_a = text_a
#         self.text_b = text_b
#         self.label = label
#
#
# def make_examples(tokenizer, x, y=None, maxlen=128, output_mode="classification", is_tf_dataset=True):
#     examples = []
#     y = y if isinstance(y, list) or isinstance(y, np.ndarray) else [None] * len(x)
#
#     for i, (_x, _y) in tqdm(enumerate(zip(x, y)), position=0, total=len(x)):
#         guid = "%s" % i
#         label = int(_y)
#
#         if isinstance(_x, str):
#             text_a = _x
#             text_b = None
#         else:
#             assert len(_x) == 2
#             text_a = _x[0]
#             text_b = _x[1]
#
#         examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#
#     features = glue_convert_examples_to_features(
#         examples,
#         tokenizer,
#         maxlen,
#         output_mode=output_mode,
#         label_list=list(np.unique(y)))
#
#     all_input_ids = []
#     all_attention_masks = []
#     all_token_type_ids = []
#     all_labels = []
#
#     for f in tqdm(features, position=0, total=len(examples)):
#         if is_tf_dataset:
#             all_input_ids.append(tf.constant(f.input_ids))
#             all_attention_masks.append(tf.constant(f.attention_mask))
#             all_token_type_ids.append(tf.constant(f.token_type_ids))
#             all_labels.append(tf.constant(f.label))
#         else:
#             all_input_ids.append(f.input_ids)
#             all_attention_masks.append(f.attention_mask)
#             all_token_type_ids.append(f.token_type_ids)
#             all_labels.append(f.label)
#
#     if is_tf_dataset:
#         dataset = tf.data.Dataset.from_tensor_slices(({
#             'input_ids': all_input_ids,
#             'attention_mask': all_attention_masks,
#             'token_type_ids': all_token_type_ids
#         }, all_labels))
#
#         return dataset, features
#
#     xdata = [np.array(all_input_ids), np.array(all_attention_masks), np.array(all_token_type_ids)]
#     ydata = all_labels
#
#     return [xdata, ydata], features
#
# train_dataset_base, train_examples = make_examples(tokenizer, x_train, y_train, maxlen=128)
# valid_dataset_base, valid_examples = make_examples(tokenizer, x_valid, y_valid, maxlen=128)
#
# test_dataset_base, test_examples = make_examples(tokenizer, x_test, y_test, maxlen=128)
# [xtest, ytest], test_examples = make_examples(tokenizer, x_test, y_test, maxlen=128, is_tf_dataset=False)
#
# for value in train_dataset_base.take(1):
#     print(f'     input_ids: {value[0]["input_ids"]}')
#     print(f'attention_mask: {value[0]["attention_mask"]}')
#     print(f'token_type_ids: {value[0]["token_type_ids"]}')
#     print(f'        target: {value[1]}')
#
# def get_training_dataset(dataset, batch_size):
#     dataset = dataset.repeat()
#     dataset = dataset.shuffle(2048)
#     dataset = dataset.batch(batch_size)
#
#     return dataset
#
# def get_validation_dataset(dataset, batch_size):
#     dataset = dataset.batch(batch_size)
#
#     return dataset
#
# train_dataset = get_training_dataset(train_dataset_base, TRAIN_BATCH_SIZE)
# valid_dataset = get_training_dataset(valid_dataset_base, VALID_BATCH_SIZE)
#
# train_steps = len(train_examples) // TRAIN_BATCH_SIZE
# valid_steps = len(valid_examples) // VALID_BATCH_SIZE
#
# train_steps, valid_steps
#
# """### Model"""
#
# def build_model(model_name, config, learning_rate=3e-5):
#     model = TFBertForSequenceClassification.from_pretrained(model_name, config=config)
#
#     optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
#     metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
#     model.compile(optimizer=optimizer, loss=loss, metrics=[metric])
#
#     return model
#
# model = build_model(MODEL_NAME_OR_PATH, config, learning_rate=LEARNING_RATE)
#
# """### Training"""
#
# # Commented out IPython magic to ensure Python compatibility.
# # %%time
# #
# # r = model.fit(
# #     train_dataset,
# #     validation_data=valid_dataset,
# #     steps_per_epoch=train_steps,
# #     validation_steps=valid_steps,
# #     epochs=EPOCHS,
# #     verbose=1)
# #
# # final_accuracy = r.history['val_accuracy']
# # print('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))
#
# # save the model
#
# model.save_pretrained(os.path.dirname(OUTPUT_PATH))
#
# """### Evaluation / Prediction"""
#
# ev = model.evaluate(test_dataset_base.batch(TEST_BATCH_SIZE))
# print()
# print(f'Evaluation: {ev}')
# print()
#
# predictions = model.predict(xtest)
# ypred = predictions[0].argmax(axis=-1).tolist()
#
# print()
# print(classification_report(ytest, ypred, target_names=labels))
# print()
#
# print(f'F1: {f1_score(ytest, ypred, average="weighted")}')
#
# """## Script"""
#
# !rm -rf /content/taaghche
# !mkdir -p /content/taaghche
#
# ntrain = train[['comment', 'label']]
# ntrain.columns = ['text', 'label']
#
# ndev = valid[['comment', 'label']]
# ndev.columns = ['text', 'label']
#
# ntest = test[['comment', 'label']]
# ntest.columns = ['text', 'label']
#
#
# ntrain.to_csv('/content/taaghche/' + 'train.tsv', sep='\t', index=False)
# ndev.to_csv('/content/taaghche/' + 'dev.tsv', sep='\t', index=False)
# ntest.to_csv('/content/taaghche/' + 'test.tsv', sep='\t', index=False)
#
# # Commented out IPython magic to ensure Python compatibility.
# !git clone https://github.com/hooshvare/parsbert.git parsbert
# # %cd /content/parsbert/scripts/
# !rm -rf __pycache__
# !ls
#
# !python run_clf_finetuning.py
#
# !rm -rf /content/bert-fa-base-uncased-sentiment-taaghceh-v2/
# !mkdir -p /content/bert-fa-base-uncased-sentiment-taaghceh-v2/
# !rm -rf /content/scripts/runs
# !rm /content/taaghche/cached_*
#
#
# !python run_clf_finetuning.py \
#     --labels positive,negative \
#     --output_mode classification \
#     --task_name taaghche \
#     --model_name_or_path HooshvareLab/bert-fa-base-uncased \
#     --data_dir /content/taaghche/ \
#     --output_dir /content/bert-fa-base-uncased-sentiment-taaghceh-v2/ \
#     --max_seq_length 128 \
#     --per_device_train_batch_size 16 \
#     --per_device_eval_batch_size 16 \
#     --learning_rate 2e-5 \
#     --num_train_epochs 3.0 \
#     --logging_steps 500 \
#     --save_steps 1000 \
#     --do_train \
#     --do_eval \
#     --do_predict \
#     --overwrite_output_dir \
#     --logging_first_step
#
torch.save(pt_model.state_dict(), 'bert_business_24class.pt')

pt_model = SentimentModel(config=config)
pt_model = pt_model.to(device)
pt_model.load_state_dict(torch.load('bert_business_24class.pt'))





# print(pt_model.state_dict()['bert.encoder.layer.11.attention.output.LayerNorm.weight'])
# pt_model.eval()

